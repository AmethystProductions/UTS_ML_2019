{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of A1 Literature Review.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmethystProductions/UTS_ML_2019/blob/master/A1_Literature_Review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1RsTM0J9BMB",
        "colab_type": "text"
      },
      "source": [
        "# Literature Review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmIaALQb6SFw",
        "colab_type": "text"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPDA8AvAwd52",
        "colab_type": "text"
      },
      "source": [
        "Long Short-Term Memory is a research paper addressing the issues that has plagued Recurrent Neural Networks by introduction the idea of Gated Recurrent Neural Networks. This has greatly reduced the time lag and inaccuracy of Recurrent Neural Networks and paved the path of many more variants of this architecture, and allowed us to progress in many field of today, such as translation and voice transcription."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWsOSCzq6Nku",
        "colab_type": "text"
      },
      "source": [
        "## Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pClStAwy6goL",
        "colab_type": "text"
      },
      "source": [
        "This research paper analyze the existing algorithms for Recurrent Neural Networks (RNNs) and points out issues in their practical usage due to two key factors: excessive time delay of vanishing gradients and exploding gradients. This is due to RNN using the same weight parameter for each layer, thus the multiplications performed on the gradients that would lead to an exponential increase or decrease in value as more and more layers.\n",
        "\n",
        "A new architecture for building RNNs was proposed by Hochreiter el al. (1997) in this paper to address the issues, called Long Short-Term Memory (LSTM). LSTM introduces a central concept called Constant Error Carousel (CEC), which is the idea that the activator produced by the RNN stays confined and does not cause  vanishing and exploding gradients.\n",
        "\n",
        "This is achieved through the use of the input and output gates. The input gate takes in the and controls what values is passed on and prevents CEC from taking irrelevant inputs. The output gate controls the values used to computer the activation. Together, these gates form a Memory Cell, and is the central component that makes up a LSTM RNN. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGAWsdp16Nth",
        "colab_type": "text"
      },
      "source": [
        "## Innovation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtgyV-LVXNoo",
        "colab_type": "text"
      },
      "source": [
        "At the time of proposal, LSTM was one of the latest idea that attempts to address the issues that has plagued RNNs. However, unlike earlier proposals like RTRL by Robinson el al. (1987) which still suffers problems with time lags, or the time constant approach by Mozer (1992) where you need manual input. LSTM addresses the problem using a self sufficient method that reduces the complexity and time. \n",
        "\n",
        "LSTM is among one of the most important architectures for RNNs, alongside Bidirectional Recurrent Neural Network (BRNN), which was proposed by Schuster and Paliwal in the same year (1997). Over the years, LSTM has become of the most widely used RNNs and has contributed to the development of projects such as Speech Recognition (Graves et al. 2013) and Image Caption Generation (Vinyals el al. 2015), having been cited over 20000 times.\n",
        "\n",
        "The proposal of LSTM is more than just the architecture itself. It opened up the idea of Gated Recurrent Neural Networks, and paved the way for other variants to follow.\n",
        "One notable example is Gated Recurrent Unit, which was proposed by Cho et al. (2014), another commonly used RNN which although less powerful individually, it's reduction in complexity allow more varies algorithms to be build to solve problems LSTM would perform worse on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzhsA8tu6NzZ",
        "colab_type": "text"
      },
      "source": [
        "## Technical Quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mvf1P821iS_",
        "colab_type": "text"
      },
      "source": [
        "The paper is of relatively high technical quality. The authors have support their theory by putting the proposed architecture in 6 experimentation which all have a focus on time lags, and was compared against existing architectures such as Real Time Recurrent Learning (RTRL) by Robinson et al. (1987), using data that was previously gathered for these experiments.\n",
        "\n",
        "The author was able to demonstrates that not only does LSTM has a higher success rate in all case when other architectures will have 50% or less success rate, it is also much faster in comparison in majority of the cases.\n",
        "\n",
        "Despite no having complete first hand data, the results for the other architectures come from reliable sources and the results were very conclusively in favour of LSTM.\n",
        "\n",
        "The authors conducted experiments with many trials and demonstrated LSTM on experiments that other architectures would have trouble attempting to solve with a high degree of success."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73wRHvyI6N6Z",
        "colab_type": "text"
      },
      "source": [
        "## Application and X-Factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxKvAwLWAWIy",
        "colab_type": "text"
      },
      "source": [
        "The architecture proposed in the paper provides strong and reasonable theory in solving the problems it intends to address, and is able to demonstrate it in the experimentation. LSTM is a perfect fit for RNNs and is designed to address belonging to RNNs specifically. There are some limitations, as mentioned in its discussions section, which would prompt students and researchers to discuss how the remaining issues can be solved. An interesting note is that some issues that were addressed in later revisions of LSTM, with the proposal of forget gate, which reduces complexity by discarding unnecessary memory storage.\n",
        "\n",
        "The idea that attracted me that most about the paper is that the concept of it is very simple and intuitive once explained, but it was able to address the problems plaguing RNNs for a very significant period of time. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoHVxWZd6OBz",
        "colab_type": "text"
      },
      "source": [
        "## Presentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFo4m_sGvzzs",
        "colab_type": "text"
      },
      "source": [
        "The papers follows a very clear structure in presenting its topics. It first points out the issues that it intends to address, then talks about the attempts that has been made to address said issues and where they fail, proposes new architecture with theory, demonstrate its practicality with experimentation, and concludes with some limitations.\n",
        "\n",
        "However, the paper goes very in depths regarding some concepts, which makes some parts of it very hard to approach, despite the architecture itself being fairly simple to understand. And although figures were employed for clarity in explaining the architectures, some of them are messy and confusing to look at, and could be better structured."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kswI7iGDySso",
        "colab_type": "text"
      },
      "source": [
        "## Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVTCOmqvyVuS",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/AmethystProductions/UTS_ML_2019/blob/master/A1_Literature_Review.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFUjOw_e6PoP",
        "colab_type": "text"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW7JDps9YJAV",
        "colab_type": "text"
      },
      "source": [
        "Hochreiter, S. and Schmidhuber, J., 1997. Long short-term memory. *Neural computation*, 9(8), pp.1735-1780."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kepvm6krwlY",
        "colab_type": "text"
      },
      "source": [
        "Cho, K., Van MerriÃ«nboer, B., Bahdanau, D. and Bengio, Y., 2014. On the properties of neural machine translation: Encoder-decoder approaches. *arXiv preprint arXiv*:1409.1259."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LnvFu-T0spba"
      },
      "source": [
        "Robinson, A.J. and Fallside, F., 1987. *The utility driven dynamic error propagation network.* Cambridge: University of Cambridge Department of Engineering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkPTZOwXvqBr",
        "colab_type": "text"
      },
      "source": [
        "Mozer, M.C., 1992. Induction of multiscale temporal structure. In *Advances in neural information processing systems* (pp. 275-282)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtM5PvO4xiYQ",
        "colab_type": "text"
      },
      "source": [
        "Schuster, M. and Paliwal, K.K., 1997. Bidirectional recurrent neural networks. *IEEE Transactions on Signal Processing*, 45(11), pp.2673-2681."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7lpX8sS0Iad",
        "colab_type": "text"
      },
      "source": [
        "Graves, A., Mohamed, A.R. and Hinton, G., 2013, May. Speech recognition with deep recurrent neural networks. In *2013 IEEE international conference on acoustics, speech and signal processing* (pp. 6645-6649). IEEE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Rg3J6kJ0lmN",
        "colab_type": "text"
      },
      "source": [
        "Vinyals, O., Toshev, A., Bengio, S. and Erhan, D., 2015. Show and tell: A neural image caption generator. In *Proceedings of the IEEE conference on computer vision and pattern recognition* (pp. 3156-3164)."
      ]
    }
  ]
}