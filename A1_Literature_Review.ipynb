{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of A1 Literature Review.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmethystProductions/UTS_ML_2019/blob/master/A1_Literature_Review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1RsTM0J9BMB",
        "colab_type": "text"
      },
      "source": [
        "# Literature Review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmIaALQb6SFw",
        "colab_type": "text"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWsOSCzq6Nku",
        "colab_type": "text"
      },
      "source": [
        "## Content (300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pClStAwy6goL",
        "colab_type": "text"
      },
      "source": [
        "This research paper analyze the existing algorithms for Recurrent Neural Networks (RNNs) and points out issues in their practical usage due to two key factors: excessive time delay and improper weighting of neural net due to exponential increase or decrease of multiplicative gradients.\n",
        "\n",
        "A new architecture for building RNNs was proposed by Hochreiter el al. (1997) in this paper to address the issues, called Long Short-Term Memory (LSTM). LSTM introduces a few central concepts to address the problems presented: Constant Error Carousel, Memory Cells and Gate Units. \n",
        "\n",
        "Input and output gate units were used to scale gradients and limit the minimum and maximum to prevent exponential increase \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGAWsdp16Nth",
        "colab_type": "text"
      },
      "source": [
        "## Innovation (300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtgyV-LVXNoo",
        "colab_type": "text"
      },
      "source": [
        "At the time of introduction of this paper, it was not immediately perceived as the revolutionary method that it is known for today. It was only when the Gated Recurrent Unit (GRU), a simplified variant of LSTM was proposed by Cho el al. (2014), LSTM also started to gain traction. Citation towards this paper and its authors (Hochreiter and Schmidhuber) siginificantly increased at around 2014 onwards and peaked at 2018.\n",
        "\n",
        "At the time of proposal, LSTM was one of the latest idea that attempts to address the issues that has plagued RNNs. However, unlike earlier proposals like the "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzhsA8tu6NzZ",
        "colab_type": "text"
      },
      "source": [
        "## Technical Quality (200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73wRHvyI6N6Z",
        "colab_type": "text"
      },
      "source": [
        "## Application and X-Factor (200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxKvAwLWAWIy",
        "colab_type": "text"
      },
      "source": [
        "The authors mentioned having conduct experiements with non-truncated LSTM but did not present the finding in the research paper for the reader to come to their own conclusion, but only mentions it in passing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoHVxWZd6OBz",
        "colab_type": "text"
      },
      "source": [
        "## Presentation (100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFUjOw_e6PoP",
        "colab_type": "text"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW7JDps9YJAV",
        "colab_type": "text"
      },
      "source": [
        "Hochreiter, S. and Schmidhuber, J., 1997. Long short-term memory. *Neural computation*, 9(8), pp.1735-1780."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kepvm6krwlY",
        "colab_type": "text"
      },
      "source": [
        "Cho, K., Van MerriÃ«nboer, B., Bahdanau, D. and Bengio, Y., 2014. On the properties of neural machine translation: Encoder-decoder approaches. *arXiv preprint arXiv*:1409.1259."
      ]
    }
  ]
}