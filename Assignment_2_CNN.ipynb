{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 2 - CNN",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmethystProductions/UTS_ML_2019/blob/master/Assignment_2_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsTmiG92a-7q",
        "colab_type": "text"
      },
      "source": [
        "# Convolution Neural Network\n",
        "\n",
        "A Convolution Neural Network (CNN) is a multilayer perception that's generally used for analysing and classifying images.\n",
        "\n",
        "\n",
        "Different CNN architectures takes different steps at different orders, for example LeNet goes through the process of:\n",
        "\n",
        "- input\n",
        "- convolution\n",
        "- pooling\n",
        "- convolution\n",
        "- pooling\n",
        "- flattened convolution\n",
        "- fully connected\n",
        "\n",
        "\n",
        "with each step taking the outputs of previous steps as input and producing a classification of the image. The result is then evaluated in training and the loss is calculated and passed back through each layer for the CNN for figure out the best filter to extract features out of the inputs.\n",
        "\n",
        "\n",
        "For this project, I'll be making a simple CNN algorithm involves the steps as follows:\n",
        "\n",
        "- input \n",
        "- convolution\n",
        "- pooling\n",
        "- fully connected\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-A-1qFlvbQk",
        "colab_type": "text"
      },
      "source": [
        "## Inputs and Outputs\n",
        "\n",
        "The inputs for a CNN is generally a series of images which are converted into a 2d array. In this project, I used the [MNIST dataset](http://yann.lecun.com/exdb/mnist/), imported from [pypi](https://pypi.org/project/mnist/) for handwritten digits as they have 10k samples, allowing me to train with great accuracy.\n",
        "\n",
        "the outputs of the program is is the prediction, loss, and accuracy of a model that can relatively accurately predict handwritten numerical characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLKFo9oD3ord",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "\n",
        "First, the required libraries are imported and setup, and a demonstration of the MNIST dataset is shown to ensure the data is correct to understand what we're working with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ4IjNMCK6Xw",
        "colab_type": "code",
        "outputId": "9eb83fd9-d6f6-4cdb-eef8-5a4483ca9615",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "!pip install mnist\n",
        "import mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_images = mnist.train_images()\n",
        "train_labels = mnist.train_labels()\n",
        "\n",
        "example = (train_labels[0], train_images[0])\n",
        "plt.imshow(example[1])\n",
        "plt.show()\n",
        "print(example[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mnist in /usr/local/lib/python3.6/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mnist) (1.16.5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5ZSIa81g1gy",
        "colab_type": "text"
      },
      "source": [
        "## Convolution\n",
        "\n",
        "First step in a CNN is to apply convolution to the input. Convolution is the process of reducing the complexity of the image and extracting the important features out of the input.\n",
        "\n",
        "### Filters\n",
        "A `filter` is used to accomplish this step, a square matrix of values of the same depth as the input is applied onto the image at every pixel (excluding the edges) and its neighbours through element-wise multiplication, with the results summed up, giving the pixel value for the new image. \n",
        "\n",
        "For a coloured image,  the filter will generally be a 3d matrix, as coloured images uses RGB channels, with means that they have a depth of 3, or 4 if RGBA. The depth of the filter should always be the same as the depth of the image.\n",
        "\n",
        "For this project, I'll be using a 3x3x1 matrix, as the inputs are simple black and white images. A typical CNN may have many filters, but since the image set is fairly simple, I'll be using 2 filter layers.\n",
        "\n",
        "#### Stride\n",
        "\n",
        "Stride decides how a filter is applied to the image, it is how many steps to skip when moving from 1 pixel to the next when applying the filters, both horizontally and vertially. A stride of more than 1 will reduce the size if the image, so it's useful for large images.\n",
        "\n",
        "A stride of 1 is called `no stride`, which is what I'll be using for this project as the images are small to begin with, and a larger stride to cause a loss of information.\n",
        "\n",
        "#### Paddings\n",
        "After the filter is applied, the resulting image will be smaller than the original image, this is typically not a problem unless the filters are relatively large. Having no padding is call `valid padding`.\n",
        "\n",
        "If the same image size was to be kept, then a padding of 0s can be added to the edge of the input image, with more paddings added depending on the matrix size, this is called `same padding`.\n",
        "\n",
        "For this project, I'll be using `valid padding` as there'll likely be no loss of information with losing a few pixels at the border.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MCqMbNI_zPX",
        "colab_type": "text"
      },
      "source": [
        "### Example of Sobel Filter applied to an image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqu-xERFlgCj",
        "colab_type": "code",
        "outputId": "f440f87a-9e99-4fcd-d118-004f32b2b0ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "sobel_filter = [\n",
        "    [-1, 0, 1],\n",
        "    [-2, 0, 2],\n",
        "    [-1, 0, 1]\n",
        "]\n",
        "\n",
        "example_h, example_w = example[1].shape\n",
        "print(\"size of original image:\", example[1].shape)\n",
        "\n",
        "example_conv = np.zeros((example_h - 2, example_w - 2))\n",
        "\n",
        "for i in range(example_h - 2): \n",
        "  for j in range(example_w - 2):\n",
        "    example_region = example[1][i:i+3, j:j+3] # taking a 3x3 image region\n",
        "    \n",
        "    # applying the filter, summing them, then apply the new value the pixel location\n",
        "    example_conv[i, j] = np.sum(example_region * sobel_filter)\n",
        "\n",
        "\n",
        "print(\"size of image after convolution:\", example_conv.shape)\n",
        "plt.imshow(example_conv)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size of original image: (28, 28)\n",
            "size of image after convolution: (26, 26)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAERFJREFUeJzt3X1o3dd9x/HP11ePlmTLsmVFdZQn\n14W5DXU6NVuX0KW0S9NQcDpY1jCKx0JdRjNaKGwhI2v+GiG0DWGMMncJcUabrtCEmJGuzUxZ1q0L\nkY2TOPX8UNePtaU48oMky3q4+u4P/TI0Rzrn9t7ffVDO+wVCV/f8dM5XV/rod+895/f7mbsLQHpW\n1LsAAPVB+IFEEX4gUYQfSBThBxJF+IFEEX4gUYQfSBThBxLVVMvBCp0d3tTTU8shgaTMjo6qOD5h\npWxbUfjN7C5JT0gqSPpHd380OFhPj/r/8iuVDAkg4MxjT5S8bdlP+82sIOnvJX1G0mZJ95nZ5nL7\nA1Bblbzmv1XSEXc/6u7Tkr4vaWs+ZQGotkrCv0HSyQVfn8ruA7AMVP3dfjPbbmZDZjZUHB+v9nAA\nSlRJ+E9LGljw9bXZff+Pu+9w90F3Hyx0dlYwHIA8VRL+VyVtMrMbzaxF0ucl7cqnLADVVvZUn7vP\nmtkDkn6s+am+p9z9zdwqA1BVFc3zu/uLkl7MqRYANcTyXiBRhB9IFOEHEkX4gUQRfiBRhB9IFOEH\nEkX4gUQRfiBRhB9IFOEHEkX4gUQRfiBRhB9IFOEHElXTi3agxvL41z4Xbvau2WgXFrmEhF8pRPto\nORfeZqbLg+0rB8aiY0yMtgfbW3/dHGxvuRi/Vsb06nCd092RBzxH7PmBRBF+IFGEH0gU4QcSRfiB\nRBF+IFGEH0gU4QcSxSKfMnhTeKGGJCm23mNFuA9rjS/28NnwICtai/E+ipH//5fCfyLtXVPRMQqF\n8M8yPt0R7WPl2fDPenF1eIxPX38gOsZzF28JtncfDP/Oug/FL0Q7/NGuYPt0d7SL3LDnBxJF+IFE\nEX4gUYQfSBThBxJF+IFEEX4gUczzL8JbwvO5fdeNRvuYicyfX7gYntte2RGfP5+83Bpsb2mdifYx\nOxuuczYyzz85Fq5Bip/Mo/1k+CQZktQ2Gp7HvxBZe7F3dCA6RtNwS7C99WJ43YQX4ifzKLZFN6mZ\nisJvZsckjUkqSpp198E8igJQfXns+T/h7udy6AdADfGaH0hUpeF3ST8xsz1mtn2xDcxsu5kNmdlQ\ncTy+9hlAbVT6tP92dz9tZuslvWRm/+PuLy/cwN13SNohSa3XDZRwRAyAWqhoz+/up7PPI5Kel3Rr\nHkUBqL6yw29mHWbW9c5tSXdK2p9XYQCqq5Kn/X2Snrf5SdwmSd9z93/Npao6s7bwfO7DH/iXaB8X\niuF5/Idf2Rpsn7gYvoCEJPlM+H/35FT8Yhi6Eu6jaTKyDqAlPkbstd41P4+vaWi6HL44yPDHwhPo\nxw9eEx2j9UrknAE3heNy4f2d0TEm+xrnlW/Z4Xf3o5I+nGMtAGqIqT4gUYQfSBThBxJF+IFEEX4g\nUYQfSBThBxLFyTwWEbsYRodNR/vY1PZ2eIxi5MQPHj8xxIr28MIXOxs/0cbqQ/FxQs7fHq5Bkrq7\nJ4LtLRfiJ/MojIYPCrNiZFFU/BoomukMbzS7MvxYeSG+gGeuuXEW+bDnBxJF+IFEEX4gUYQfSBTh\nBxJF+IFEEX4gUczzL8KuhE9Q8diJu6J9fLTneHiDufCc8fUD8bOh/07vsWD7rkO/F+2j76dng+2T\nG9cG289Hfg5J6u0Iz/OPDMYvqLHqZHge31dE5s9L2M3FZ+AbZ44+D+z5gUQRfiBRhB9IFOEHEkX4\ngUQRfiBRhB9IFPP8i4kc+33gyIZoF0e61oU3mAr/392y9lR0jId7/zvY/kLTx6J9FA8fDW9wU0+w\n2S/HL9rxy7O94Q3eH58/n+yLHfMfvtAK3o09P5Aowg8kivADiSL8QKIIP5Aowg8kivADiSL8QKJY\n5FMGm4z/z5ydCp98wiILiX49uTo6RueKtmB710fCFw6RpELf+mB72/DlYHvTpXidxchFUNQSX+Qz\ntZZFPHmL/hWb2VNmNmJm+xfc12NmL5nZ4ezzmuqWCSBvpTztf1rS1eetelDSbnffJGl39jWAZSQa\nfnd/WdLoVXdvlbQzu71T0j051wWgysp9w6/P3c9kt89K6ltqQzPbbmZDZjZUHA9fbBFA7VT8br+7\nuwKnNXX3He4+6O6Dhc7OSocDkJNywz9sZv2SlH0eya8kALVQbvh3SdqW3d4m6YV8ygFQK9F5fjN7\nVtIdktaZ2SlJX5f0qKQfmNn9ko5LureaRS5LkXn8mL0n4heyeGZd+IQhf7f52Wgff/5HfxFs738p\n/KSufSR+0Y6ZzvAJP+Za4/P8M10VPqB4l2j43f2+JZo+mXMtAGqI5b1Aogg/kCjCDySK8AOJIvxA\nogg/kCjCDySKk3k0qLm3W6PbPH7wU8H2nR9+OtrHbdv2BNv/ve23g+2F6egQWnMovEDH4+uENLo5\nvJ+a7QwvFPJCfCFRatjzA4ki/ECiCD+QKMIPJIrwA4ki/ECiCD+QKOb5G1UJ09IXTq8Ktv9N99Zo\nH3/2vv8Ij/GH4YuP/NehjdExmv6zJdjesz98YRBJsrlwHRdvCu/HrvTGTwbiTWmtBWDPDySK8AOJ\nIvxAogg/kCjCDySK8AOJIvxAopjnX8ZsNnwg/GuH4xf++MaVTwfbP953JNi+7ub4xVd3r/5AsH24\nozvax/o9k8H21gvhC4Ocuzm81kCSJq4rRrd5L2HPDySK8AOJIvxAogg/kCjCDySK8AOJIvxAogg/\nkKjoIh8ze0rSZyWNuPuHsvsekfRFSW9lmz3k7i9Wq0iUxy6HF75I0slf9Qbbnx9fGWzfuO7t6BhP\nbtkZbP+H/k9E+3ht4uZge9+PTgTb+8fDP6ck/ao3fMKQYnv8hCDLSSl7/qcl3bXI/Y+7+5bsg+AD\ny0w0/O7+sqTRGtQCoIYqec3/gJm9bmZPmdma3CoCUBPlhv/bkjZK2iLpjKRvLrWhmW03syEzGyqO\nxw8CAVAbZYXf3Yfdvejuc5K+I+nWwLY73H3Q3QcLnZ3l1gkgZ2WF38z6F3z5OUn78ykHQK2UMtX3\nrKQ7JK0zs1OSvi7pDjPbovmzyx+T9KUq1gigCqLhd/f7Frn7ySrUggZ0ZTJ8Eoyjb6+N9vG+G6eC\n7X+87pVoHz+7PjzPv35qOtjedPBkdIzCVPikI8XwMoBlhxV+QKIIP5Aowg8kivADiSL8QKIIP5Ao\nwg8kivADieKKPcuYN3uwfUXXTLSPtd0Twfae9svB9otTbdEx/nb4U8H2ExPxg0ILU+GrE/lE+OdY\n0RMfo9ga3eQ9hT0/kCjCDySK8AOJIvxAogg/kCjCDySK8AOJYp6/XmLX0+iYjXbRs3Ys2L6p51y0\nj5YV4XFOjofnx0cvhS/qIUk/OhU+EUfLSPzPcNWpyJqG3vBJRS7/1jXRMd5rF+WIYc8PJIrwA4ki\n/ECiCD+QKMIPJIrwA4ki/ECimOcvR2yOXpI3h+eMO3vDx59vWvtWdIw1LZPB9rHZ+AHqrw1vCLZf\nOtsVbG87E/8T6ng73N56Pj6/XpgOz/Of+/1rg+1jA+HzAcwrlrDNewd7fiBRhB9IFOEHEkX4gUQR\nfiBRhB9IFOEHEkX4gURFV2iY2YCkZyT1SXJJO9z9CTPrkfTPkm6QdEzSve5+vnql5ijyL6/QMxVs\nb18Zbpek2dnwSqAPrj8bbF/dHF7AI0lDwwPB9tHhVdE+2k61hOuILNBpK2GBTuR8IbrSHV+AM7Um\n/EubXh1eBFRcmdYCnlKUsueflfQ1d98s6XclfdnMNkt6UNJud98kaXf2NYBlIhp+dz/j7nuz22OS\nDkjaIGmrpJ3ZZjsl3VOtIgHk7zd6zW9mN0i6RdIrkvrc/UzWdFbzLwsALBMlh9/MOiX9UNJX3f3S\nwjZ3d82/H7DY9203syEzGyqOj1dULID8lBR+M2vWfPC/6+7PZXcPm1l/1t4vaWSx73X3He4+6O6D\nhc7OPGoGkINo+M3MJD0p6YC7f2tB0y5J27Lb2yS9kH95AKqllOP5b5P0BUlvmNm+7L6HJD0q6Qdm\ndr+k45LurU6JAKohGn53/5mkpSZiP5lvOXHeEp7PbVo1He2jqTk85/vBa84E29sK8Qtq7DsbPknG\nwXPrg+0Tl+Mn4tDx8AUzeo7Gu2g7H34s5prDc/BTq+Jz9NORefzp7vDvVJJmmafPHSv8gEQRfiBR\nhB9IFOEHEkX4gUQRfiBRhB9I1LK7aEfXNWPB9j/ZOBTt48jl8Bz74Yu9wfaRS/FlytPHw9tciUxt\nN03E/y93ngh30jEcnxufXhUe59L14faZyHH0kjTbHjnm3+J9IH/s+YFEEX4gUYQfSBThBxJF+IFE\nEX4gUYQfSBThBxK17Bb5dLdfCbZf2zIa7ePnozcF24+fWBdsL5xvjo7RPhI+gcVse/j7myeiQ8gi\na2cubIr/eqe6w+0zq8OD+AoW6CxX7PmBRBF+IFGEH0gU4QcSRfiBRBF+IFGEH0jUspvnPz0Snph+\nbPLOaB9jF8IXu7DLhXAHJZx8YnpVuN0jSwXm4ksJNB25YEZsjl5inj5l7PmBRBF+IFGEH0gU4QcS\nRfiBRBF+IFGEH0gU4QcSFV3kY2YDkp6R1CfJJe1w9yfM7BFJX5T0VrbpQ+7+YrUKfYefbwm2j0Xa\n8zDXEl8YU8o2QD2VssJvVtLX3H2vmXVJ2mNmL2Vtj7v7N6pXHoBqiYbf3c9IOpPdHjOzA5I2VLsw\nANX1G73mN7MbJN0i6ZXsrgfM7HUze8rM1uRcG4AqKjn8ZtYp6YeSvurulyR9W9JGSVs0/8zgm0t8\n33YzGzKzoeL4eA4lA8hDSeE3s2bNB/+77v6cJLn7sLsX3X1O0nck3brY97r7DncfdPfBQmf80tYA\naiMafjMzSU9KOuDu31pwf/+CzT4naX/+5QGollLe7b9N0hckvWFm+7L7HpJ0n5lt0fz03zFJX6pK\nhQCqwtxrNx9tZm9JOr7grnWSztWsgPJRZ76WQ53LoUbp3XVe7+69pXxjTcP/rsHNhtx9sG4FlIg6\n87Uc6lwONUqV1cnyXiBRhB9IVL3Dv6PO45eKOvO1HOpcDjVKFdRZ19f8AOqn3nt+AHVSt/Cb2V1m\ndtDMjpjZg/WqI8bMjpnZG2a2z8yG6l3PO7LjKUbMbP+C+3rM7CUzO5x9ruvxFkvU+IiZnc4ez31m\ndnc9a8xqGjCzn5rZL8zsTTP7SnZ/oz2eS9VZ1mNal6f9ZlaQdEjSH0g6JelVSfe5+y9qXkyEmR2T\nNOjuDTXna2YflzQu6Rl3/1B232OSRt390ewf6hp3/6sGq/ERSeONdCh4tlq1f+Fh65LukfSnaqzH\nc6k671UZj2m99vy3Sjri7kfdfVrS9yVtrVMty5K7vyxp9Kq7t0ramd3eqfk/jLpZosaG4+5n3H1v\ndntM0juHrTfa47lUnWWpV/g3SDq54OtTatxzBLikn5jZHjPbXu9iIvqy8y9I0lnNn32pETXsoeBX\nHbbesI9nHofX84Zf3O3u/hFJn5H05eypbMPz+ddzjTiVU9Kh4PWwyGHr/6eRHs9yD6+/Wr3Cf1rS\nwIKvr83uazjufjr7PCLpeS1x6HKDGH7naMvs80id63mXUg8Fr7XFDltXAz6elRxef7V6hf9VSZvM\n7EYza5H0eUm76lTLksysI3tjRWbWIelONfahy7skbctub5P0Qh1rWVQjHgq+1GHrarDHM/fD6929\nLh+S7tb8O/6/lPTX9aojUuNNkl7LPt5spDolPav5p3gzmn/P5H5JayXtlnRY0r9J6mnAGv9J0huS\nXtd8uPob4LG8XfNP6V+XtC/7uLsBH8+l6izrMWWFH5Ao3vADEkX4gUQRfiBRhB9IFOEHEkX4gUQR\nfiBRhB9I1P8CWU8RM8IUXtQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS5vi1aA8tdi",
        "colab_type": "text"
      },
      "source": [
        "### Feed Forward\n",
        "\n",
        "Once the filter is applied, the output image is then feed forward into the next layer, and the input is remembered for later, once the loss is calculated.\n",
        "\n",
        "### Back Propagation\n",
        "\n",
        "Because this is the first layer in our neural network, there's nothing further back to pass it to. However, it still takes information regarding the loss gradient of this layer's output and apply it to the filter weight. Because the filter is applied to all pixels of the image, any small change in its weighting changes what the output value is once the filter is applied to the original image. \n",
        "\n",
        "The size of the change is determined by the `learning rate`, which scales the changes to a degree that is suitable for the CNN to learn. A smaller learning rate would mean more precision, but also more time spent training, while too large of a learning rate may never reach its target."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-pd-m8c2bUQ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Convolution Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nwYamAIRwDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Convolution:\n",
        "\n",
        "  def __init__(self, num_filters, filter_size=None):\n",
        "    # defaults to a filter size of 3\n",
        "    if filter_size == None:\n",
        "      filter_size = 3\n",
        "    \n",
        "    self.filter_size = filter_size\n",
        "    self.num_filters = num_filters\n",
        "\n",
        "    # filters: 3d array of dimensions (num_filters, filter_size, filter_size)\n",
        "    # divide by filter_size^2 to reduce the variance of our initial values\n",
        "    self.filters = \\\n",
        "    np.random.randn(num_filters, filter_size, filter_size) / (filter_size ** 2)\n",
        "\n",
        "  def iterate_regions(self, image):\n",
        "    '''\n",
        "    Generates all possible 3x3 image regions using valid padding.\n",
        "    - image is a 2d numpy array\n",
        "    '''\n",
        "    h, w = image.shape\n",
        "\n",
        "    for i in range(h - self.filter_size + 1):\n",
        "      for j in range(w - self.filter_size + 1):\n",
        "        im_region = image[i:(i + self.filter_size), j:(j + self.filter_size)]\n",
        "        \n",
        "        # Using a generator instead of returning an array for memory efficiency\n",
        "        yield im_region, i, j \n",
        "\n",
        "  def forward(self, input):\n",
        "    '''\n",
        "    Performs a forward pass of the conv layer using the given input.\n",
        "    Returns a 3d numpy array with dimensions (h, w, num_filters).\n",
        "    - input is a 2d numpy array\n",
        "    '''\n",
        "    self.last_input = input\n",
        "    \n",
        "    h, w = input.shape\n",
        "    output = \\\n",
        "    np.zeros((h - self.filter_size + 1, w - self.filter_size + 1, self.num_filters))\n",
        "\n",
        "    for im_region, i, j in self.iterate_regions(input):\n",
        "      output[i, j] = np.sum(im_region * self.filters, axis=(1, 2))\n",
        "    return output\n",
        "  \n",
        "  def backprop(self, loss_out, learn_rate):\n",
        "    '''\n",
        "    Performs a backward pass of the convolution layer\n",
        "    loss_out: a 3d numpy array, the loss gradient of this layer's outputs\n",
        "    learn_rate: percision in the steps to take when learning\n",
        "    '''\n",
        "    d_L_d_filters = np.zeros(self.filters.shape)\n",
        "\n",
        "    for im_region, i, j in self.iterate_regions(self.last_input):\n",
        "      for f in range(self.num_filters):\n",
        "        d_L_d_filters[f] += loss_out[i, j, f] * im_region\n",
        "\n",
        "    # Update filters\n",
        "    self.filters -= learn_rate * d_L_d_filters\n",
        "\n",
        "    # CNN first layer, does not go back further\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wVrlRUog1r4",
        "colab_type": "text"
      },
      "source": [
        "## Pooling\n",
        "\n",
        "Pooling is the process of downsampling, reducing redundancy in the image pixel information, as neighbouring pixels tends to be of similar values. \n",
        "\n",
        "There are generally 3 types of pooling:\n",
        "\n",
        "- max pooling\n",
        "- min pooling\n",
        "- avarage pooling\n",
        "\n",
        "All these are essentially applying a filter over generally non-overlapping regions. \n",
        "\n",
        "In our case, `max pooling` with a 2x2 filter and stride 2, the max value of 2x2 region is taken as assigned as new new pixel value, and then the filter is applied 2 steps away horizontally and veritically until all pixels are pooled.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1wvbMt9Ns-5",
        "colab_type": "text"
      },
      "source": [
        "### Pooling 2x2 with Stride 2 Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uqlIys_NyD5",
        "colab_type": "code",
        "outputId": "3acb14df-56e4-45f5-882a-cece0a364257",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "example_h, example_w = example_conv.shape\n",
        "example_h = example_h // 2\n",
        "example_w = example_w // 2\n",
        "\n",
        "print(\"size of original image:\", example_conv.shape)\n",
        "\n",
        "example_pool = np.zeros((example_h, example_w))\n",
        "\n",
        "for i in range(example_h): \n",
        "  for j in range(example_w):\n",
        "    # taking a 2x2 image region with stride 2\n",
        "    example_region = example[1][(2 * i):(2 * i + 2), (2 * j):(2 * j + 2)] \n",
        "    \n",
        "    # get the max value in the array\n",
        "    example_pool[i, j] = np.amax(example_region)\n",
        "\n",
        "\n",
        "print(\"size of image after pooling:\", example_pool.shape)\n",
        "plt.imshow(example_pool)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size of original image: (26, 26)\n",
            "size of image after pooling: (13, 13)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\no2JvSnrjAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yBJwhyC1WLd",
        "colab_type": "text"
      },
      "source": [
        "### Feed Forward\n",
        "\n",
        "Takes the input from the convolution layer and applies 2x2 max pooling on each non-overlapping image region, resulting in halving the size of the image, but the same depth.\n",
        "\n",
        "### Back Propagation\n",
        "\n",
        "Max pooling actually does not use any weights as it directly takes the max values of each region, however, it is still necessary to assign the loss gradient to where the orginal max value was so that it can be used by the convolutional layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbPA8zm92mcS",
        "colab_type": "text"
      },
      "source": [
        "### Pooling Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSLmtjeQOv_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Pooling:\n",
        "  # A Max Pooling layer using a pool size of 2\n",
        "\n",
        "  def iterate_regions(self, image):\n",
        "    '''\n",
        "    Generates non-overlapping 2x2 image regions to pool over.\n",
        "    - image is a 2d numpy array\n",
        "    '''\n",
        "    h, w, _ = image.shape\n",
        "    new_h = h // 2\n",
        "    new_w = w // 2\n",
        "\n",
        "    for i in range(new_h):\n",
        "      for j in range(new_w):\n",
        "        im_region = image[(i * 2):(i * 2 + 2), (j * 2):(j * 2 + 2)]\n",
        "        yield im_region, i, j\n",
        "  \n",
        "  def forward(self, input):\n",
        "    '''\n",
        "    Performs a forward pass of the maxpool layer using the given input.\n",
        "    Returns a 3d numpy array with dimensions (h / 2, w / 2, num_filters).\n",
        "    - input is a 3d numpy array with dimensions (h, w, num_filters)\n",
        "    '''\n",
        "    self.last_input = input\n",
        "      \n",
        "    h, w, num_filters = input.shape\n",
        "    output = np.zeros((h // 2, w // 2, num_filters))\n",
        "\n",
        "    for im_region, i, j in self.iterate_regions(input):\n",
        "      output[i, j] = np.amax(im_region, axis=(0, 1))\n",
        "    return output\n",
        "\n",
        "  def backprop(self, d_L_d_out):\n",
        "    '''\n",
        "    Performs a backward pass of the maxpool layer.\n",
        "    Returns the loss gradient for this layer's inputs.\n",
        "    - d_L_d_out is the loss gradient for this layer's outputs.\n",
        "    '''\n",
        "    d_L_d_input = np.zeros(self.last_input.shape)\n",
        "\n",
        "    for im_region, i, j in self.iterate_regions(self.last_input):\n",
        "      h, w, f = im_region.shape\n",
        "      amax = np.amax(im_region, axis=(0, 1))\n",
        "\n",
        "      for i2 in range(h):\n",
        "        for j2 in range(w):\n",
        "          for f2 in range(f):\n",
        "            # If this pixel was the max value, copy the gradient to it.\n",
        "            if im_region[i2, j2, f2] == amax[f2]:\n",
        "              d_L_d_input[i * 2 + i2, j * 2 + j2, f2] = d_L_d_out[i, j, f2]\n",
        "    \n",
        "    return d_L_d_input\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P03YP99g5SJ",
        "colab_type": "text"
      },
      "source": [
        "## Fully Connected Layer\n",
        "\n",
        "The fully connected layer takes the output from the pooling layer and produces a vector of N size, where N is the number of classes. Each value in the vector represents the probability of of the input being of that class.\n",
        "\n",
        "The class with the highest probability is chosen as the final prediction, but having the probability value for the actual correct class allows us to better calculate loss. \n",
        "\n",
        "There are many ways to calculate loss, two most notable being Mean Squared Error (MSE) and Cross-Entropy loss. Cross-Entropy is overall better performing for CNNs, being a more punishing loss calculation, which is what I'll be using.\n",
        "\n",
        "### Cross-Entropy Loss\n",
        "\n",
        "Cross-entropy loss is also known as log loss. It measures the performance of probability values between 0-1. The loss increases exponentially the lower the correct prediction is. This is a crucial step in letting the CNN understand what it did wrong and how much it is wrong by, using the current weights.\n",
        "\n",
        "It's calculated by `L=-ln(p)`, where `L` is loss, and `p` is predicted probability of the correct class.\n",
        "\n",
        "\n",
        "### Softmax Activation Function\n",
        "\n",
        "Softmax is a function that converts a vector of values into to the range of 0-1, with the sum adding up to one, which allows to calculate for the cross entropy loss of the predictions\n",
        "\n",
        "It does so by raising each value(`i`) in the vector in `e^i`, using that as the numerator, and then using the sum of `e^i` for all values in the vector as the demoniator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM19fn-jsxgL",
        "colab_type": "text"
      },
      "source": [
        "### Softmax Example with Loss\n",
        "\n",
        "This is akin to random guessing as the weights are not properly trained, and there are no biases towards any value based on training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odzHrBY5s8Jc",
        "colab_type": "code",
        "outputId": "032f6f7e-ff41-4a19-fd9d-3af5c50079a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "example_softmax = example_pool.flatten() # flattened as the extra dimentionality are not useful\n",
        "\n",
        "# 10 classes, one for each digit\n",
        "example_weights = np.random.randn(example_softmax.size, 10) / example_softmax.size \n",
        "\n",
        "example_totals = np.dot(example_softmax, example_weights)\n",
        "\n",
        "# raise all values by exponential\n",
        "example_exp = np.exp(example_totals)\n",
        "# divide each individual value in respect to sum\n",
        "example_classification = example_exp / np.sum(example_exp)\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "print(\"vector of predictions:\\n\", example_classification)\n",
        "\n",
        "np.set_printoptions(precision=None, suppress=False)\n",
        "print(\"prediction:\", np.argmax(example_classification))\n",
        "print(\"actual:\", example[0])\n",
        "\n",
        "# L = -log(p)\n",
        "example_loss = -np.log(example_classification[example[0]]) \n",
        "print(\"loss:\", example_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vector of predictions:\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "prediction: 6\n",
            "actual: 5\n",
            "loss: 18.7156095927266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvNBr6qE4wuf",
        "colab_type": "text"
      },
      "source": [
        "### Feed Forward\n",
        "\n",
        "The input is flattened, and a value is assigned to each of the 10 classes based on the weights and biases. The prediction is scaled by softmax activation and the result is returned.\n",
        "\n",
        "### Back Propagation\n",
        "\n",
        "A vector is passed into function with only theintial gradient for predictions for the correct class containing the value of the softmax outputs. \n",
        "\n",
        "The loss gradient is then calculated for all other classes through taking the derivative of the correct class prediction over the the respective classes with the equation `-e^t_c * -e^t_k / S^2`, where `e^t_c` is the exponential for the correct class, and `e^t_k`is the exponential for the incorrect classes, and S is the sum of the exponentials.\n",
        "\n",
        "The loss gradient for the correct class is gotten through `e^t_c * (S - e^t_c) / S^2`, with the same variables as above.\n",
        "\n",
        "After getting the loss gradient for each prediction, they are then used to calculate the following:\n",
        "\n",
        "- weight graidnet, `d_L/d_w`\n",
        "- biases gradient, `d_L/d_b`\n",
        "- input gradient, `d_L/d_input`\n",
        "\n",
        "the weight gradients and biases gradients are used to update the softmax activation's values, while the input gradient is passed alone into the pooling layer for further breakdown."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1ZUbFtys88u",
        "colab_type": "text"
      },
      "source": [
        "### Softmax Activation Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HilJ8G45ZkQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Softmax:\n",
        "  # A standard fully-connected layer with softmax activation.\n",
        "\n",
        "  def __init__(self, input_len, nodes):\n",
        "    # We divide by input_len to reduce the variance of our initial values\n",
        "    self.weights = np.random.randn(input_len, nodes) / input_len\n",
        "    self.biases = np.zeros(nodes)\n",
        "\n",
        "  def forward(self, input):\n",
        "    '''\n",
        "    Performs a forward pass of the softmax layer using the given input.\n",
        "    Returns a 1d numpy array containing the respective probability values.\n",
        "    - input can be any array with any dimensions.\n",
        "    '''\n",
        "    self.last_input_shape = input.shape\n",
        "    \n",
        "    input = input.flatten()\n",
        "    self.last_input = input\n",
        "    input_len, nodes = self.weights.shape\n",
        "\n",
        "    # Makes prediction\n",
        "    totals = np.dot(input, self.weights) + self.biases\n",
        "    self.last_totals = totals\n",
        "    \n",
        "    # Scales prediction by e^totals\n",
        "    exp = np.exp(totals)\n",
        "    # e^totals / sum(e^totals)\n",
        "    return exp / np.sum(exp, axis=0)\n",
        "  \n",
        "  def backprop(self, d_L_d_out, learn_rate):    \n",
        "    '''\n",
        "    Performs a backward pass of the softmax layer.\n",
        "    Returns the loss gradient for this layer's inputs.\n",
        "    - d_L_d_out is the loss gradient for this layer's outputs.\n",
        "    - learn_rate is a float    \n",
        "    '''\n",
        "    # We know only 1 element of d_L_d_out will be nonzero\n",
        "    for i, gradient in enumerate(d_L_d_out):\n",
        "      if gradient == 0:\n",
        "        continue\n",
        "\n",
        "      # e^totals\n",
        "      t_exp = np.exp(self.last_totals)\n",
        "\n",
        "      # Sum of all e^totals\n",
        "      S = np.sum(t_exp)\n",
        "\n",
        "      # Gradients of out[i] against totals\n",
        "      d_out_d_t = -t_exp[i] * t_exp / (S ** 2)\n",
        "      d_out_d_t[i] = t_exp[i] * (S - t_exp[i]) / (S ** 2)\n",
        "\n",
        "      # Gradients of totals against weights/biases/input\n",
        "      d_t_d_w = self.last_input\n",
        "      d_t_d_b = 1\n",
        "      d_t_d_inputs = self.weights\n",
        "\n",
        "      # Gradients of loss against totals\n",
        "      d_L_d_t = gradient * d_out_d_t\n",
        "\n",
        "      # Gradients of loss against weights/biases/input\n",
        "      d_L_d_w = d_t_d_w[np.newaxis].T @ d_L_d_t[np.newaxis]\n",
        "      d_L_d_b = d_L_d_t * d_t_d_b\n",
        "      d_L_d_inputs = d_t_d_inputs @ d_L_d_t\n",
        "\n",
        "      # Update weights / biases      \n",
        "      self.weights -= learn_rate * d_L_d_w      \n",
        "      self.biases -= learn_rate * d_L_d_b       \n",
        "      return d_L_d_inputs.reshape(self.last_input_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_XApUFIAAlQ",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "Training is arguably the most crucial part as it ties everything together, and teaches the CNN on what to look for and what values to have in the filters and weights.\n",
        "\n",
        "The training process is done through `back propagation`, where the loss is calculated for the predictions and passed back through each layer for them to readjust their weight values towards better accuracy. \n",
        "\n",
        "The more accurate their predictions are, the less they would have to adjust in value, due to the nature of Cross-Entropy loss.\n",
        "\n",
        "The training is done in 3 `epochs`, which means 3 forward pass and backwards pass of all the examples. With each epoch containing a thousand training samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaL7mwmcfm8i",
        "colab_type": "text"
      },
      "source": [
        "### Training Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXi4r49J9BxR",
        "colab_type": "code",
        "outputId": "b8a3a8e6-c873-4760-eeab-12829e2310a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "# Using the first thousand for demonstration\n",
        "train_images = mnist.train_images()[:1000]\n",
        "train_labels = mnist.train_labels()[:1000]\n",
        "test_images = mnist.test_images()[:1000]\n",
        "test_labels = mnist.test_labels()[:1000]\n",
        "\n",
        "num_filters = 3\n",
        "\n",
        "conv = Convolution(num_filters)                  # 28x28x1 -> 26x26x8\n",
        "pool = Pooling()                  # 26x26x8 -> 13x13x8\n",
        "softmax = Softmax(13 * 13 * num_filters, 10) # 13x13x8 -> 10\n",
        "\n",
        "def forward(image, label):\n",
        "  '''\n",
        "  Completes a forward pass of the CNN and calculates the accuracy and\n",
        "  cross-entropy loss.\n",
        "  - image is a 2d numpy array\n",
        "  - label is a digit\n",
        "  '''\n",
        "  # We transform the image from [0, 255] to [-0.5, 0.5] to make it easier\n",
        "  # to work with. This is standard practice.\n",
        "  out = conv.forward((image / 255) - 0.5)\n",
        "  out = pool.forward(out)\n",
        "  out = softmax.forward(out)\n",
        "\n",
        "  # Calculate cross-entropy loss and accuracy\n",
        "  loss = -np.log(out[label])\n",
        "  acc = 1 if np.argmax(out) == label else 0\n",
        "\n",
        "  return out, loss, acc\n",
        "\n",
        "def train(im, label, lr=.005):\n",
        "  '''\n",
        "  Completes a full training step on the given image and label.\n",
        "  Returns the cross-entropy loss and accuracy.\n",
        "  - image is a 2d numpy array\n",
        "  - label is a digit\n",
        "  - lr is the learning rate\n",
        "  '''\n",
        "  out, loss, acc = forward(im, label)\n",
        "\n",
        "  # Calculate initial gradient\n",
        "  gradient = np.zeros(10)\n",
        "  gradient[label] = -1 / out[label]\n",
        "\n",
        "  # Backprop\n",
        "  gradient = softmax.backprop(gradient, lr)\n",
        "  gradient = pool.backprop(gradient)\n",
        "  gradient = conv.backprop(gradient, lr)\n",
        "\n",
        "  return loss, acc\n",
        "\n",
        "\n",
        "# Train the CNN for 3 epochs\n",
        "for epoch in range(3):\n",
        "  print('--- Epoch %d ---' % (epoch + 1))\n",
        "\n",
        "  # Shuffle the training data\n",
        "  permutation = np.random.permutation(len(train_images))\n",
        "  train_images = train_images[permutation]\n",
        "  train_labels = train_labels[permutation]\n",
        "\n",
        "  # Train!\n",
        "  loss = 0\n",
        "  num_correct = 0\n",
        "  for i, (im, label) in enumerate(zip(train_images, train_labels)):\n",
        "    if i > 0 and i % 100 == 99:\n",
        "      print(\n",
        "        '[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%' %\n",
        "        (i + 1, loss / 100, num_correct)\n",
        "      )\n",
        "      loss = 0\n",
        "      num_correct = 0\n",
        "\n",
        "    l, acc = train(im, label)\n",
        "    loss += l\n",
        "    num_correct += acc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Epoch 1 ---\n",
            "[Step 100] Past 100 steps: Average Loss 2.274 | Accuracy: 12%\n",
            "[Step 200] Past 100 steps: Average Loss 2.198 | Accuracy: 22%\n",
            "[Step 300] Past 100 steps: Average Loss 2.009 | Accuracy: 34%\n",
            "[Step 400] Past 100 steps: Average Loss 1.458 | Accuracy: 59%\n",
            "[Step 500] Past 100 steps: Average Loss 1.172 | Accuracy: 62%\n",
            "[Step 600] Past 100 steps: Average Loss 0.839 | Accuracy: 77%\n",
            "[Step 700] Past 100 steps: Average Loss 0.748 | Accuracy: 77%\n",
            "[Step 800] Past 100 steps: Average Loss 0.632 | Accuracy: 77%\n",
            "[Step 900] Past 100 steps: Average Loss 0.684 | Accuracy: 77%\n",
            "[Step 1000] Past 100 steps: Average Loss 0.879 | Accuracy: 71%\n",
            "--- Epoch 2 ---\n",
            "[Step 100] Past 100 steps: Average Loss 0.561 | Accuracy: 83%\n",
            "[Step 200] Past 100 steps: Average Loss 0.586 | Accuracy: 77%\n",
            "[Step 300] Past 100 steps: Average Loss 0.655 | Accuracy: 82%\n",
            "[Step 400] Past 100 steps: Average Loss 0.470 | Accuracy: 83%\n",
            "[Step 500] Past 100 steps: Average Loss 0.577 | Accuracy: 82%\n",
            "[Step 600] Past 100 steps: Average Loss 0.474 | Accuracy: 85%\n",
            "[Step 700] Past 100 steps: Average Loss 0.502 | Accuracy: 84%\n",
            "[Step 800] Past 100 steps: Average Loss 0.680 | Accuracy: 79%\n",
            "[Step 900] Past 100 steps: Average Loss 0.519 | Accuracy: 80%\n",
            "[Step 1000] Past 100 steps: Average Loss 0.567 | Accuracy: 82%\n",
            "--- Epoch 3 ---\n",
            "[Step 100] Past 100 steps: Average Loss 0.361 | Accuracy: 85%\n",
            "[Step 200] Past 100 steps: Average Loss 0.537 | Accuracy: 84%\n",
            "[Step 300] Past 100 steps: Average Loss 0.420 | Accuracy: 87%\n",
            "[Step 400] Past 100 steps: Average Loss 0.459 | Accuracy: 88%\n",
            "[Step 500] Past 100 steps: Average Loss 0.477 | Accuracy: 86%\n",
            "[Step 600] Past 100 steps: Average Loss 0.580 | Accuracy: 82%\n",
            "[Step 700] Past 100 steps: Average Loss 0.294 | Accuracy: 91%\n",
            "[Step 800] Past 100 steps: Average Loss 0.452 | Accuracy: 84%\n",
            "[Step 900] Past 100 steps: Average Loss 0.367 | Accuracy: 87%\n",
            "[Step 1000] Past 100 steps: Average Loss 0.430 | Accuracy: 91%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF2-yjaBvetl",
        "colab_type": "code",
        "outputId": "49ef58f4-005b-4f28-bf21-d406ed830e7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Test the CNN\n",
        "print('\\n--- Testing the CNN ---')\n",
        "loss = 0\n",
        "num_correct = 0\n",
        "for im, label in zip(test_images, test_labels):\n",
        "  _, l, acc = forward(im, label)\n",
        "  loss += l\n",
        "  num_correct += acc\n",
        "\n",
        "num_tests = len(test_images)\n",
        "print('Test Loss:', loss / num_tests)\n",
        "print('Test Accuracy:', num_correct / num_tests)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "--- Testing the CNN ---\n",
            "Test Loss: 0.524555431820456\n",
            "Test Accuracy: 0.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZO6tbm36dsN",
        "colab_type": "code",
        "outputId": "984ebdfd-a3bb-4681-ed44-669d41c2babd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pred, l, acc = forward(example[1], example[0])\n",
        "\n",
        "print(\"predicted:\", np.argmax(pred))\n",
        "print(\"actual:\", example[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted: 5\n",
            "actual: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1SALdFo6KSm",
        "colab_type": "text"
      },
      "source": [
        "# Summary\n",
        "\n",
        "Although the CNN does not produce the most accurate result, and there's still a lot to improve on, the success of this CNN implementation is undeniable. With an accuracy of around 90%, the prediction has a lot to improve still, but even so, it works well enough for its intended purpose.\n",
        "\n",
        "Some improvements could be made with the structure of the CNN, possibly following AlexNet structure or LeNet would prove to produce better results, or perhaps simply adding more layers would improve the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TIyPI7SnIvA",
        "colab_type": "text"
      },
      "source": [
        "# Social and Ethical Complications\n",
        "\n",
        "General applications of a Convolutional Neural Network is in the genre related to image classification, this can also include object detection, facial recognition and so on. \n",
        "\n",
        "There's a lot of good that it can do and has done for society so far, with one example being Optical Character Recognition (OCR), which is not too different from what this project does. OCR has significantly improved with CNNs and allows for much simplier and streamlined process for processing handwritten documents. \n",
        "\n",
        "CNN's application in medical imaging is very notable, as a well trained CNN algorithm is able to classify imagings containing signs of lung disease (Q. Li et al. 2014), with different applications of the algorithms being able achieve many things for the medical field.\n",
        "\n",
        "However, with advancement of technology comes abuse of said technology. CNN's potential for misuse is quite high. Just recently there's 2 apps developed using Generative Adversarial Networks (GANs) called DeepFake and DeepNude. Both applications allows the user to unconcentially replace the image of someone with something different, while keeping certain recognisable characteristics, with DeepNude being more malicious in its design than DeepFake. Both applications, although not directly utilising CNNs, demonstrates the possible misuse in image classification techniques such as CNN. This is not to mention the possibilities of using facial recognition implemented with CNN for unconsential monitoring of other.\n",
        "\n",
        "Algorithms like CNN ultimately does more good in making people easier and potentially saving people's lives. But when implementing such algorithms, it's imperitive to review your intentions, following the Kantian ideal of ethics. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbi4M4kHoET8",
        "colab_type": "text"
      },
      "source": [
        "# References\n",
        "\n",
        "Q. Li, W. Cai, X. Wang, Y. Zhou, D. D. Feng and M. Chen, \"Medical image classification with convolutional neural network,\" *2014 13th International Conference on Control Automation Robotics & Vision (ICARCV)*, Singapore, 2014, pp. 844-848.\n",
        "\n",
        "Zhou, V. (2019). CNNs, Part 1: An Introduction to Convolutional Neural Networks. [online] Victorzhou.com. Available at: *https://victorzhou.com/blog/intro-to-cnns-part-1/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhfVOj7xo6bd",
        "colab_type": "text"
      },
      "source": [
        "# Links\n",
        "\n",
        "Github:\n",
        "https://github.com/AmethystProductions/UTS_ML_2019/blob/master/Assignment_2_CNN.ipynb\n",
        "\n",
        "Youtube:\n",
        "https://youtu.be/BI0gjb2KKWM"
      ]
    }
  ]
}